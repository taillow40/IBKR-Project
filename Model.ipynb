{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gym.wrappers import FlattenObservation\n",
    "from gym.spaces.utils import unflatten\n",
    "import numpy as np\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import TD3\n",
    "import torch\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import argparse\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from AdvancedStockGame import AdvancedStockGame\n",
    "from AdvancedStockGameTD3 import AdvancedStockGameTD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMNetwork(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim):\n",
    "        super(CustomLSTMNetwork, self).__init__(observation_space, features_dim)\n",
    "\n",
    "\n",
    "        stock_space = observation_space.spaces['stock_data']\n",
    "        \n",
    "        self.num_stocks = stock_space.shape[0]\n",
    "        self.seq_length = stock_space.shape[1]\n",
    "        self.n_features = stock_space.shape[2]\n",
    "\n",
    "        portfolio_space = observation_space.spaces['portfolio']\n",
    "        self.portfolio_dim = portfolio_space.shape[0]\n",
    "\n",
    "        portfolio_value_space = observation_space.spaces['portfolio_value']\n",
    "        self.portfolio_value_dim = 10\n",
    "        \n",
    "        stock_layer_size = 1024\n",
    "\n",
    "        portfolio_layer_size = 32 \n",
    "\n",
    "        portfolio_value_layer_size = 32\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=self.n_features * self.num_stocks, num_layers=3, hidden_size=stock_layer_size, batch_first=True)\n",
    "\n",
    "        self.portfolio_layer = nn.Linear(self.portfolio_dim, portfolio_layer_size)  \n",
    "\n",
    "        self.portfolio_value_layer = nn.Linear(self.portfolio_value_dim, portfolio_value_layer_size)\n",
    "\n",
    "        self.final_layer = nn.Linear(stock_layer_size + portfolio_layer_size + portfolio_value_layer_size, features_dim)  \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, observations):\n",
    "        main_input = observations['stock_data'].view(-1, self.seq_length, self.num_stocks * self.n_features)\n",
    "        lstm_out, _ = self.lstm(main_input)\n",
    "        lstm_out = lstm_out[:, -1, :] \n",
    "        \n",
    "        portfolio_input = observations['portfolio']\n",
    "        portfolio_out = self.relu(self.portfolio_layer(portfolio_input))\n",
    "\n",
    "        portfolio_value_input = observations['portfolio_value']\n",
    "       # portfolio_value_out = self.relu(self.portfolio_value_layer(portfolio_value_input))\n",
    "        portfolio_value_out = self.relu(self.portfolio_value_layer(portfolio_value_input)).squeeze(1)\n",
    "        #print(f\"lstm_out shape: {lstm_out.shape}\")\n",
    "       # print(f\"portfolio_out shape: {portfolio_out.shape}\")\n",
    "       # print(f\"portfolio_value_out shape: {portfolio_value_out.shape}\")\n",
    "\n",
    "        combined = torch.cat((lstm_out, portfolio_out, portfolio_value_out), dim=1)  \n",
    "        combined = self.relu(combined)\n",
    "\n",
    "\n",
    "        return self.final_layer(combined)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model(data_file_path):\n",
    "    policy_kwargs = dict (\n",
    "        features_extractor_class=CustomLSTMNetwork,\n",
    "        features_extractor_kwargs=dict(500)\n",
    "    )\n",
    "    model = DQN(\n",
    "    policy='MlpPolicy',  # Use the same MLP policy architecture\n",
    "    env=FlattenObservation(AdvancedStockGame(data_file_path)),  # Your custom environment\n",
    "    learning_rate=0.0001,  # Learning rate, similar to PPO\n",
    "    buffer_size=16384,  # Size of the replay buffer\n",
    "    learning_starts=1000000,  # Number of environment steps to collect before learning starts\n",
    "    batch_size=128,  # Similar to PPO\n",
    "    tau=0.01,  # The soft update coefficient ('Polyak update', 1.0 means hard update)\n",
    "    gamma=1,  # Discount factor, similar to PPO\n",
    "    train_freq=4,  # Update the model every `train_freq` steps\n",
    "    gradient_steps=1,  # How many gradient update steps to take after each batch of steps\n",
    "    optimize_memory_usage=False,  # Optimize memory usage by reducing data precision\n",
    "    target_update_interval=16384,  # Update the target network every `target_update_interval` steps\n",
    "    exploration_fraction=0.2,  # Fraction of entire training period over which the exploration rate is reduced\n",
    "    exploration_initial_eps=1.0,  # Initial value of random action probability\n",
    "    exploration_final_eps=0.1,  # Final value of random action probability\n",
    "    max_grad_norm=2,  # Maximum norm for the gradient clipping\n",
    "    tensorboard_log=None,  # Directory for Tensorboard logs\n",
    "    policy_kwargs=policy_kwargs,  # Custom policy arguments\n",
    "    verbose=0,  # Verbosity level\n",
    "    seed=40,  # Seed for the pseudo-random generators\n",
    "    device=\"cuda\",  # Device to use for PyTorch (either 'cuda' or 'cpu')\n",
    "    _init_setup_model=True  # Whether or not to build the network at the creation of the instance\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model(data_file_path):\n",
    "    policy_kwargs = dict (\n",
    "        features_extractor_class=CustomLSTMNetwork,\n",
    "        features_extractor_kwargs=dict(features_dim=500)\n",
    "    )\n",
    "    model = A2C(\n",
    "        policy='MultiInputPolicy', \n",
    "        env=AdvancedStockGame(data_file_path),  \n",
    "        n_steps=5,\n",
    "        gamma=1,\n",
    "        gae_lambda=1.0,\n",
    "        ent_coef=0.01,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=2,\n",
    "        use_rms_prop=False,\n",
    "        use_sde=False,\n",
    "        sde_sample_freq=-1,\n",
    "        normalize_advantage=False,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=0,\n",
    "        seed=40,\n",
    "        device=\"cuda\",\n",
    "        _init_setup_model=True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = {\n",
    "    \"net_arch\": {\n",
    "        \"pi\": [2048, 2048],\n",
    "        \"vf\": [2048, 2048],\n",
    "    },\n",
    "}\n",
    "def new_model(data_file_path):\n",
    "    model = PPO(\n",
    "        policy='MultiInputPolicy',  # Use the same MLP policy architecture\n",
    "        env=AdvancedStockGame(data_file_path, window_size=4096),  # Your custom environment\n",
    "        learning_rate=0.0000000001,  # Learning rate\n",
    "        n_steps=32768,  # Number of steps to run for each environment per update\n",
    "        batch_size=32768,  # Batch size for the optimization process\n",
    "        n_epochs=3,  # Number of epochs to optimize for each update cycle\n",
    "        gamma=1,  # Discount factor\n",
    "        gae_lambda=0.95,  # Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "        clip_range=0.2,  # Clip parameter for PPO\n",
    "        ent_coef=0.01,  # Entropy coefficient for the loss calculation\n",
    "        vf_coef=0.5,  # Value function coefficient for the loss calculation\n",
    "        max_grad_norm=2,  # Maximum norm for the gradient clipping\n",
    "        use_sde=False,  # Whether to use State Dependent Exploration\n",
    "        sde_sample_freq=-1,  # Sample a new noise matrix every n steps when using gSDE\n",
    "        target_kl=None,  # Target KL divergence threshold for early stopping\n",
    "        tensorboard_log=None,  # Directory for Tensorboard logs\n",
    "        policy_kwargs=policy_kwargs,  # Custom policy arguments\n",
    "        verbose=0,  # Verbosity level\n",
    "        seed=40,  # Seed for the pseudo-random generators\n",
    "        device=\"cuda\",  # Device to use for PyTorch (either 'cuda' or 'cpu')\n",
    "        _init_setup_model=True  # Whether or not to build the network at the creation of the instance\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def new_model(data_file_path):\n",
    "    model = TD3(\n",
    "        policy='MultiInputPolicy',  # TD3 uses MLP (Multi-Layer Perceptron) policy by default\n",
    "        env=AdvancedStockGameTD3(data_file_path),  # Your custom environment\n",
    "        learning_rate=0.001,  # Typical starting learning rate for TD3\n",
    "        buffer_size=2000,  # Size of the replay buffer\n",
    "        learning_starts=2000,  # Number of steps before learning starts\n",
    "        batch_size=64,  # Batch size for learning\n",
    "        tau=0.005,  # Polyak averaging coefficient for updating the target network\n",
    "        gamma=0.99,  # Discount factor for future rewards\n",
    "        train_freq=1000,  # Train the model every `train_freq` steps\n",
    "        gradient_steps=-1,  # Update the model as many times as steps done during the episode\n",
    "        action_noise=None,  # Action noise added to the target policy during training\n",
    "        replay_buffer_class=None,  # Custom replay buffer class\n",
    "        replay_buffer_kwargs=None,  # Custom replay buffer class arguments\n",
    "        optimize_memory_usage=False,  # Enable a memory efficient variant of the replay buffer\n",
    "        policy_kwargs=None,  # Policy specific arguments\n",
    "        tensorboard_log=None,  # Tensorboard log directory\n",
    "        verbose=0,  # Verbosity level\n",
    "        seed=40,  # Random seed\n",
    "        device=\"cuda\",  # PyTorch device (cuda or cpu)\n",
    "        _init_setup_model=True  # Whether to initialize the model at creation\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, env):\n",
    "    obs, info = env.reset()\n",
    "    portfolio = []\n",
    "    actions = []\n",
    "    total_reward = 0\n",
    "    i = 0\n",
    "    while True:  \n",
    "        try:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "        except:\n",
    "            print(obs)\n",
    "            break\n",
    "        obs, reward, done, term, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        actions.append(action)\n",
    "        portfolio.append(env.get_portfolio_value())\n",
    "        i += 1\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "\n",
    "\n",
    "    action, _states = model.predict(obs,deterministic=True)\n",
    "    actions.append(action)\n",
    "\n",
    "    return actions, portfolio, total_reward\n",
    "env = AdvancedStockGame(\"full_data.csv\")\n",
    "model = new_model(\"full_data.csv\")\n",
    "\n",
    "evaluate(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = new_model(\"full_data.csv\")\n",
    "model.learn(total_timesteps=10_000)\n",
    "model.save('models/Cripplew4096/Cripplew4096')\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1 (tags/v3.12.1:2305ca5, Dec  7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9240d949b7e875368571ba59acc67192d2efbcc4561b3c6f94c83d7858e18732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
